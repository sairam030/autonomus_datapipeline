<p align="center">
  <h1 align="center">ğŸš€ Autonomous Data Pipeline</h1>
  <p align="center">
    <strong>AI-Powered, Configuration-Driven Data Pipeline with Medallion Architecture</strong>
  </p>
  <p align="center">
    <em>Upload â†’ Detect Schema â†’ Bronze â†’ Silver â†’ Gold â†’ PostgreSQL â€” all from a visual UI.</em>
  </p>
  <p align="center">
    <img src="https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white" alt="FastAPI"/>
    <img src="https://img.shields.io/badge/React-61DAFB?style=for-the-badge&logo=react&logoColor=black" alt="React"/>
    <img src="https://img.shields.io/badge/Apache_Spark-E25A1C?style=for-the-badge&logo=apachespark&logoColor=white" alt="Spark"/>
    <img src="https://img.shields.io/badge/Apache_Airflow-017CEE?style=for-the-badge&logo=apacheairflow&logoColor=white" alt="Airflow"/>
    <img src="https://img.shields.io/badge/MinIO-C72E49?style=for-the-badge&logo=minio&logoColor=white" alt="MinIO"/>
    <img src="https://img.shields.io/badge/PostgreSQL-4169E1?style=for-the-badge&logo=postgresql&logoColor=white" alt="PostgreSQL"/>
    <img src="https://img.shields.io/badge/Apache_Kafka-231F20?style=for-the-badge&logo=apachekafka&logoColor=white" alt="Kafka"/>
    <img src="https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white" alt="Docker"/>
    <img src="https://img.shields.io/badge/Gemini_AI-8E75B2?style=for-the-badge&logo=googlegemini&logoColor=white" alt="Gemini AI"/>
  </p>
</p>

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture Diagram](#architecture-diagram)
- [Demo Video](#demo-video)
- [Tech Stack](#tech-stack)
- [Features](#features)
- [Project Structure](#project-structure)
- [Getting Started](#getting-started)
- [Service Ports](#service-ports)
- [Usage Workflow](#usage-workflow)
- [API Endpoints](#api-endpoints)
- [Environment Variables](#environment-variables)

---

## Overview

**Autonomous Data Pipeline** is a full-stack, AI-assisted data engineering platform that automates the entire data pipeline lifecycle â€” from raw data ingestion to enriched, query-ready tables â€” using the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold).

Users interact through a **React-based dark-themed UI** to upload data files, auto-detect schemas, configure AI-powered transformations via natural language chat (powered by **Google Gemini**), and orchestrate everything through auto-generated **Airflow DAGs**.

---

## Architecture Diagram
![Architecture Diagram](pic/design.png)

---

## Demo Video

<video controls width="100%" poster="screenshots/1_project_home.png">
      <source src="screenshots/automation.mp4" type="video/mp4" />
      Your browser does not support the video tag.
</video>

[Download demo video](screenshots/automation.mp4)


### Data Flow

```
  ğŸ“ Upload Files (CSV/JSON/Parquet) â”€â”€â”€â”€â”€â”
  ğŸŒ REST API Sources â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  ğŸ“¡ Apache Kafka Streams â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
  ğŸ” Auto Schema Detection (pandas + heuristic inference)
        â”‚
        â–¼
  âœ… Confirm Schema â†’ Trigger Bronze Ingestion
        â”‚
        â–¼
  ğŸ¥‰ BRONZE LAYER (MinIO: s3a://bronze/<project>/v<N>/...)
  â”‚     Raw data stored as Parquet, partitioned by ingestion date
        â”‚
        â–¼
  ğŸ¤– AI Chat (Gemini) â†’ Generate PySpark transform() code
  â”‚     Dry-run â†’ Confirm â†’ Version control
        â”‚
        â–¼
  ğŸ¥ˆ SILVER LAYER (MinIO: s3a://silver/<project>/...)
  â”‚     Cleaned, validated, transformed data
        â”‚
        â–¼
  ğŸ¤– AI Chat (Gemini) â†’ Generate Gold enrichment code
  â”‚     Joins, aggregations, API enrichment
        â”‚
        â–¼
  ğŸ¥‡ GOLD LAYER (MinIO: s3a://gold/<project>/...)
  â”‚     Business-ready, enriched datasets
        â”‚
        â–¼
  ğŸ˜ Push to PostgreSQL (append / replace / fail modes)
        â”‚
        â–¼
  âš™ï¸  Auto-generate Airflow DAGs (Bronze â†’ Silver â†’ Gold â†’ Master)
```

---

## Tech Stack

### Backend
| Technology | Version | Purpose |
|---|---|---|
| **Python** | 3.11 | Core language |
| **FastAPI** | 0.109.0 | REST API framework |
| **SQLAlchemy** | Latest | ORM for PostgreSQL |
| **Pydantic** | v2 | Settings & request/response validation |
| **Uvicorn** | 0.27.0 | ASGI server |

### Frontend
| Technology | Version | Purpose |
|---|---|---|
| **React** | 18.2 | UI framework |
| **TypeScript** | 4.9 | Type-safe JavaScript |
| **Material UI (MUI)** | 5.15 | Component library (dark theme) |
| **React Router** | 6.21 | Client-side routing |
| **TanStack React Query** | 5.17 | Server state management |
| **Monaco Editor** | 4.7 | In-browser code editor |
| **Axios** | 1.6 | HTTP client |

### Data Processing
| Technology | Version | Purpose |
|---|---|---|
| **Apache Spark** | 3.5.1 | Distributed data processing |
| **PySpark** | 3.5.1 | Python API for Spark |
| **Pandas** | 2.2.0 | Schema detection & small data |
| **PyArrow** | Latest | Parquet read/write |

### Orchestration
| Technology | Version | Purpose |
|---|---|---|
| **Apache Airflow** | 2.9.0 | Workflow orchestration |
| **LocalExecutor** | â€” | Single-node task execution |

### Storage
| Technology | Version | Purpose |
|---|---|---|
| **PostgreSQL** | 15 (Alpine) | Metadata store, Gold layer export |
| **MinIO** | Latest | S3-compatible object storage (Bronze/Silver/Gold) |
| **Redis** | 7 (Alpine) | Caching layer |

### AI / LLM
| Technology | Model | Purpose |
|---|---|---|
| **Google Gemini API** | gemini-2.5-flash | AI-powered PySpark code generation for Silver & Gold transformations |

### Streaming
| Technology | Version | Purpose |
|---|---|---|
| **Apache Kafka** | 7.6.0 (CP) | Real-time data streaming |
| **Zookeeper** | 7.6.0 (CP) | Kafka cluster coordination |
| **kafka-python-ng** | 2.2.3 | Python Kafka consumer/producer |

### Infrastructure
| Technology | Purpose |
|---|---|
| **Docker & Docker Compose** | Containerized deployment (13 services) |
| **Hadoop AWS (3.3.4)** | S3A filesystem connector for Spark |
| **PostgreSQL JDBC** | Spark â†” PostgreSQL connector |

---

## Features

- âœ… **Auto Schema Detection** â€” Upload CSV/JSON/Parquet files and auto-detect column types, nullability, and sample values with confidence scoring
- âœ… **Medallion Architecture** â€” Bronze (raw) â†’ Silver (cleaned) â†’ Gold (enriched) data layers stored in MinIO
- âœ… **AI-Powered Transformations** â€” Chat with Google Gemini to generate PySpark transformation code using natural language
- âœ… **Conversational AI Workflow** â€” Iterative code generation with clarifying questions, dry-runs, and version control
- âœ… **Visual Code Editor** â€” Edit generated PySpark code directly in the browser using Monaco Editor
- âœ… **Dry-Run Validation** â€” Test transformations on sample data before committing
- âœ… **Transformation Versioning** â€” Track, rollback, and reorder transformation steps
- âœ… **Auto DAG Generation** â€” Generate Airflow DAGs (Bronze, Silver, Gold, Master) from templates
- âœ… **PostgreSQL Export** â€” Push Gold-layer data to PostgreSQL tables (append/replace/fail modes)
- âœ… **Full Audit Trail** â€” Every generated code file is timestamped and saved to `generated_queries/`
- âœ… **Multi-Source Support** â€” CSV, JSON, Parquet, REST API, Kafka, and Database sources
- âœ… **Kafka Streaming** â€” Real-time data ingestion from Kafka topics with configurable consumer groups
- âœ… **Push to PostgreSQL** â€” Gold-layer data exported to PostgreSQL tables for BI/dashboards
- âœ… **Spark Cluster** â€” Dedicated Spark Master + Worker for distributed processing
- âœ… **Dark-Themed UI** â€” Modern React + MUI dark theme with project-based navigation
- âœ… **Modular Service Layer** â€” Clean separation of routers (HTTP) from services (business logic)

---

## Project Structure

```
autonomous_pipeline/
â”œâ”€â”€ backend/                        # FastAPI backend application
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ main.py                 # Application entry point & CORS config
â”‚       â”œâ”€â”€ config.py               # Pydantic settings (env-based config)
â”‚       â”œâ”€â”€ database.py             # SQLAlchemy session management
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â””â”€â”€ models.py           # SQLAlchemy ORM models (20 tables)
â”‚       â”œâ”€â”€ routers/                # Thin HTTP layer â€” request handling only
â”‚       â”‚   â”œâ”€â”€ pipelines.py        # Project CRUD + Kafka connectivity
â”‚       â”‚   â”œâ”€â”€ schemas.py          # Schema detection & Bronze ingestion
â”‚       â”‚   â”œâ”€â”€ bronze.py           # Bronze ingestion status & callbacks
â”‚       â”‚   â”œâ”€â”€ silver.py           # Silver transformation endpoints
â”‚       â”‚   â”œâ”€â”€ gold.py             # Gold transformation + PG push endpoints
â”‚       â”‚   â”œâ”€â”€ upload.py           # File upload management
â”‚       â”‚   â””â”€â”€ dags.py             # Airflow DAG generation & management
â”‚       â”œâ”€â”€ schemas/                # Pydantic request/response schemas
â”‚       â””â”€â”€ services/               # Business logic layer
â”‚           â”œâ”€â”€ ai_service.py       # Gemini AI integration
â”‚           â”œâ”€â”€ sandbox.py          # Safe exec() for user/AI code (sandboxed)
â”‚           â”œâ”€â”€ spark_utils.py      # Shared Spark session builders & MinIO I/O
â”‚           â”œâ”€â”€ silver_service.py   # Silver upload execution engine
â”‚           â”œâ”€â”€ gold_service.py     # Gold upload + push-to-Postgres engine
â”‚           â”œâ”€â”€ schema_detection.py # Type inference & schema analysis
â”‚           â”œâ”€â”€ dag_generator.py    # Template-based DAG rendering (6 templates)
â”‚           â”œâ”€â”€ minio_service.py    # MinIO client utilities
â”‚           â””â”€â”€ code_saver.py       # Generated code persistence
â”œâ”€â”€ frontend/                       # React TypeScript frontend
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.tsx                 # Routes & dark theme config
â”‚       â”œâ”€â”€ components/
â”‚       â”‚   â””â”€â”€ Layout.tsx          # Sidebar + AppBar layout
â”‚       â”œâ”€â”€ pages/
â”‚       â”‚   â”œâ”€â”€ ProjectsPage.tsx        # Project listing
â”‚       â”‚   â”œâ”€â”€ CreateProjectPage.tsx   # New project wizard
â”‚       â”‚   â”œâ”€â”€ ProjectDetailPage.tsx   # Project dashboard + push-to-PG
â”‚       â”‚   â”œâ”€â”€ CreateTaskPage.tsx      # DAG task creation
â”‚       â”‚   â”œâ”€â”€ SchemaPreviewPage.tsx   # Schema review & confirm
â”‚       â”‚   â”œâ”€â”€ SilverEnrichmentPage.tsx # AI chat + code editor
â”‚       â”‚   â””â”€â”€ GoldEnrichmentPage.tsx  # AI chat + code editor
â”‚       â””â”€â”€ services/
â”‚           â””â”€â”€ api.ts              # Axios API client
â”œâ”€â”€ dags/                           # Auto-generated Airflow DAGs (gitignored)
â”œâ”€â”€ data/                           # Sample / reference data files
â”œâ”€â”€ db/
â”‚   â””â”€â”€ init.sql                    # PostgreSQL schema (20 tables)
â”œâ”€â”€ engine/                         # Execution engine modules
â”œâ”€â”€ generated_queries/              # Audit trail of all generated code
â”œâ”€â”€ logs/                           # Airflow task logs (gitignored)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ spark_master_entrypoint.sh
â”‚   â””â”€â”€ kafka_producer.py           # Test Kafka producer script
â”œâ”€â”€ docker-compose.yml              # Full stack orchestration (13 services)
â”œâ”€â”€ Dockerfile                      # Custom Airflow + Spark + Python image
â”œâ”€â”€ .env.example                    # Template for environment variables
â””â”€â”€ README.md
```

---

## Getting Started

### Prerequisites

| Requirement | Minimum Version |
|---|---|
| **Docker** | 24.0+ |
| **Docker Compose** | v2.20+ |
| **Google Gemini API Key** | Required for AI features |
| **RAM** | 8 GB+ recommended |
| **Disk** | 10 GB+ free space |

### 1. Clone the Repository

```bash
git clone https://github.com/sairam030/autonomus_datapipeline.git
cd autonomous_pipeline
```

### 2. Set Environment Variables

Copy the example env file and fill in your Gemini API key:

```bash
cp .env.example .env
# Edit .env and set your GEMINI_API_KEY
```

The `.env.example` includes all configurable ports, credentials, and the Gemini API key placeholder.

### 3. Build and Start All Services

```bash
# Build the custom Docker image and start all 13 services
docker compose up -d --build
```

> â³ **First run takes ~5â€“10 minutes** to build the image, download Spark/Hadoop JARs, initialize the database, create Kafka topics, and install frontend dependencies.

### 4. Verify Services Are Running

```bash
docker compose ps
```

You should see all containers in a `healthy` or `running` state:

```
NAME                  STATUS
ap-postgres           healthy
ap-minio              healthy
ap-redis              healthy
ap-zookeeper          healthy
ap-kafka              healthy
ap-spark-master       healthy
ap-spark-worker-1     running
ap-backend            running
ap-airflow-webserver  healthy
ap-airflow-scheduler  healthy
ap-frontend           running
```

### 5. Access the Applications

| Service | URL | Credentials |
|---|---|---|
| **Frontend UI** | [http://localhost:3001](http://localhost:3001) | â€” |
| **Backend API Docs** | [http://localhost:8000/docs](http://localhost:8000/docs) | â€” |
| **Airflow UI** | [http://localhost:8085](http://localhost:8085) | `admin` / `admin` |
| **MinIO Console** | [http://localhost:9011](http://localhost:9011) | `minioadmin` / `minioadmin` |
| **Spark Master UI** | [http://localhost:8090](http://localhost:8090) | â€” |

### 6. Stop All Services

```bash
docker compose down
```

To also remove persistent volumes (database, MinIO data, etc.):

```bash
docker compose down -v
```

---

## Service Ports

| Service | Host Port | Container Port | Notes |
|---|---|---|---|
| PostgreSQL | `5433` | `5432` | Metadata & Gold export |
| MinIO API | `9010` | `9000` | S3-compatible API |
| MinIO Console | `9011` | `9001` | Web UI |
| Redis | `6380` | `6379` | Cache |
| Spark Master | `7078` | `7077` | Cluster manager |
| Spark Master UI | `8090` | `8080` | Web UI |
| Spark Worker UI | `8091` | `8081` | Web UI |
| FastAPI Backend | `8000` | `8000` | REST API |
| Airflow UI | `8085` | `8080` | Web UI |
| Kafka (External) | `9093` | `9093` | External listener |
| Kafka (Internal) | â€” | `9092` | Container-to-container |
| Zookeeper | `2181` | `2181` | Kafka coordination |
| React Frontend | `3001` | `3000` | Dev server |

---

## Usage Workflow

### Step 1: Create a Project
Open the **Frontend UI** at `http://localhost:3001`, click **"New Project"**, provide a name, description, and source type (CSV, JSON, etc.).

### Step 2: Upload Data Files
Navigate to the project and upload your data files (CSV, JSON, or Parquet).

### Step 3: Detect & Confirm Schema
Click **"Detect Schema"** to auto-detect column names, types, and statistics. Review the schema preview and click **"Confirm"** to trigger Bronze ingestion.

### Step 4: Silver Transformations (AI-Powered)
Navigate to **Silver Enrichment** and chat with the AI:
> *"Filter out rows where status is cancelled and add a column for flight duration in minutes"*

The AI generates PySpark code. **Dry-run** it on sample data, iterate, then **confirm** the version.

### Step 5: Gold Transformations (AI-Powered)
Navigate to **Gold Enrichment** for aggregations, joins, and business-level transformations.

### Step 6: Push to PostgreSQL
Export Gold-layer data to a PostgreSQL table with append/replace/fail modes.

### Step 7: Generate Airflow DAGs
Auto-generate Bronze, Silver, Gold, and Master DAGs. Enable them in the Airflow UI for scheduled execution.

---

## API Endpoints

### Pipelines
| Method | Endpoint | Description |
|---|---|---|
| `POST` | `/api/pipelines/` | Create a new pipeline |
| `GET` | `/api/pipelines/` | List all pipelines |
| `GET` | `/api/pipelines/{id}` | Get pipeline details |
| `PUT` | `/api/pipelines/{id}` | Update pipeline |
| `DELETE` | `/api/pipelines/{id}` | Delete pipeline |

### Schema & Bronze
| Method | Endpoint | Description |
|---|---|---|
| `POST` | `/api/schemas/detect` | Auto-detect schema |
| `POST` | `/api/schemas/confirm` | Confirm schema & ingest |
| `GET` | `/api/bronze/{id}/preview` | Preview Bronze data |

### Silver Transformations
| Method | Endpoint | Description |
|---|---|---|
| `POST` | `/api/silver/{id}/transformations` | Create transformation |
| `POST` | `/api/silver/{id}/transformations/{tid}/chat` | AI chat |
| `POST` | `/api/silver/{id}/transformations/{tid}/dry-run` | Dry-run code |
| `POST` | `/api/silver/{id}/transformations/{tid}/confirm` | Confirm version |
| `POST` | `/api/silver/{id}/upload-to-silver` | Execute pipeline |

### Gold Transformations
| Method | Endpoint | Description |
|---|---|---|
| `POST` | `/api/gold/{id}/transformations` | Create transformation |
| `POST` | `/api/gold/{id}/transformations/{tid}/chat` | AI chat |
| `POST` | `/api/gold/{id}/refresh-schema` | Refresh schema from Silver |
| `POST` | `/api/gold/{id}/upload-to-gold` | Execute pipeline |
| `POST` | `/api/gold/{id}/push-to-postgres` | Export to PostgreSQL |
| `GET` | `/api/gold/{id}/preview` | Preview Gold data |
| `GET` | `/api/gold/{id}/postgres-pushes` | Push history |

### Kafka & Data Sources
| Method | Endpoint | Description |
|---|---|---|
| `POST` | `/api/pipelines/kafka/test-connection` | Test Kafka connectivity |
| `POST` | `/api/pipelines/kafka/topics` | List available Kafka topics |

### DAGs
| Method | Endpoint | Description |
|---|---|---|
| `POST` | `/api/dags/{id}/generate` | Generate all DAGs |
| `GET` | `/api/dags/{id}` | List generated DAGs |
| `DELETE` | `/api/dags/{id}/{dag_id}` | Delete a DAG file |

> ğŸ“– Full interactive API docs available at **[http://localhost:8000/docs](http://localhost:8000/docs)**

---

## Environment Variables

| Variable | Default | Description |
|---|---|---|
| `GEMINI_API_KEY` | â€” | Google Gemini API key (required for AI features) |
| `GEMINI_MODEL` | `gemini-2.5-flash` | Gemini model to use |
| `DATABASE_URL` | `postgresql+psycopg2://pipeline:pipeline123@postgres:5432/autonomous_pipeline` | PostgreSQL connection |
| `MINIO_ENDPOINT` | `http://minio:9000` | MinIO endpoint |
| `AWS_ACCESS_KEY_ID` | `minioadmin` | MinIO access key |
| `AWS_SECRET_ACCESS_KEY` | `minioadmin` | MinIO secret key |
| `SPARK_MASTER_URL` | `local[*]` | Spark master URL |
| `REDIS_URL` | `redis://redis:6379/0` | Redis connection |
| `BACKEND_URL` | `http://backend:8000` | Backend URL (used by Airflow DAGs) |
| `POSTGRES_HOST` | `postgres` | PostgreSQL host for Gold push |
| `POSTGRES_PORT` | `5432` | PostgreSQL port for Gold push |
| `POSTGRES_USER` | `pipeline` | PostgreSQL user for Gold push |
| `POSTGRES_PASSWORD` | `pipeline123` | PostgreSQL password for Gold push |
| `POSTGRES_DB` | `autonomous_pipeline` | PostgreSQL database for Gold push |
| `AIRFLOW_UID` | `1000` | Airflow user ID |

---

## License

This project is for educational and development purposes.

---

